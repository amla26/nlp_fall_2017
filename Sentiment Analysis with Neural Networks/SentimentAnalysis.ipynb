{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis using Neural Networks\n",
    "\n",
    "\n",
    "In this notebook we will perform sentiment analysis using a few simple Neural Network based architectures.\n",
    "For this problem we use the IMDB Large Movie Review Dataset. The dataset contains 25,000 highly polar movie reviews for both train and test dataset, each with 12,500 positive (greater than equal to 7/10 rating) and 12,500 negative reviews(less than equal to 4/10 rating). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed, Lambda, Activation, LSTM, Flatten, Convolution1D, GRU, MaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "#from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#we retrieve train and test file names\n",
    "\n",
    "train_dir = \"./aclImdb/train/\"\n",
    "test_dir = \"./aclImdb/test/\"\n",
    "\n",
    "tr_review = [re_filename for re_filename in listdir(train_dir)]\n",
    "te_review = [re_filename for re_filename in listdir(test_dir)]\n",
    "\n",
    "#we initialize the train and test arrays\n",
    "\n",
    "tr_X = []\n",
    "tr_Y = []\n",
    "te_X = []\n",
    "te_Y = []\n",
    "\n",
    "#we arrange the reviews into the train and test arrays \n",
    "\n",
    "for review_file in tr_review:\n",
    "    f_review = open(train_dir+review_file, \"r\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    tr_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        tr_Y.append(1)\n",
    "    else:\n",
    "        tr_Y.append(0)\n",
    "    \n",
    "\n",
    "for review_file in te_review:\n",
    "    f_review = open(test_dir+review_file, \"r\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    te_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        te_Y.append(1)\n",
    "    else:\n",
    "        te_Y.append(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create the validation set from the train set\n",
    "\n",
    "Using the last 4 numbers of my uni for the seed value seed to ensure all answers remain unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#replace 2 (SEED) with the last 4 numbers of your Uni\n",
    "#Uni: as5196\n",
    "SEED = 5196\n",
    "seed_counter = 0\n",
    "while(1):\n",
    "\n",
    "    shuffle_combine = list(zip(tr_X, tr_Y))\n",
    "    random.seed(SEED+seed_counter)\n",
    "    seed_counter+=1\n",
    "    random.shuffle(shuffle_combine)\n",
    "\n",
    "    tr_X, tr_Y = zip(*shuffle_combine)\n",
    "\n",
    "    val_X = tr_X[:5000]\n",
    "    val_Y = tr_Y[:5000]\n",
    "\n",
    "    counter = 0\n",
    "    for label in val_Y:\n",
    "        counter+=label\n",
    "\n",
    "    print (counter)\n",
    "    print (seed_counter)\n",
    "    if(counter>2400 and counter <2600):\n",
    "        tr_X = tr_X[5000:]\n",
    "        tr_Y = tr_Y[5000:]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train review set : 20000\n",
      "Length of Train label set : 20000\n",
      "Length of Validation review set : 5000\n",
      "Length of Validation label set : 5000\n",
      "Length of Test review set : 25000\n",
      "Length of Test label set : 25000\n",
      "*****************************************\n",
      "Some sample Reviews Train sets and their labels\n",
      "This movie is fun to watch. If you liked \"Dave\" with Kevin Klein, you will get a kick out of this. Think \"Dave\" gone South American as Dreyfus plays J\n",
      "1\n",
      "My Tutor Friend is a well scripted romance comedy movie that has something similar to My Sassy Girl.. there's no kissing/sex scenes. Hollywood should \n",
      "1\n",
      "A meteorite falls in the country of a small town, bringing a jelly creature. An old farmer is attacked by the alien in his hand, and the youths Steve \n",
      "1\n",
      "Very rarely does one come across an indie comedy that leaves a lasting impression. Cross Eyed is a rare gem. The writer director not only tackled the \n",
      "1\n",
      "Well, there's no real plot to speak of, it's just an excuse to show some scenes of extreme violence and gratuitous sex (which can sometimes be fun, to\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Train review set : \" + str(len(tr_X)))\n",
    "print(\"Length of Train label set : \" + str(len(tr_Y)))\n",
    "print(\"Length of Validation review set : \" + str(len(val_X)))\n",
    "print(\"Length of Validation label set : \" + str(len(val_Y)))\n",
    "print(\"Length of Test review set : \" + str(len(te_X)))\n",
    "print(\"Length of Test label set : \" + str(len(te_Y)))\n",
    "print(\"*****************************************\")\n",
    "print(\"Some sample Reviews Train sets and their labels\")\n",
    "print(tr_X[0][:150])\n",
    "print(tr_Y[0])\n",
    "print(tr_X[1][:150])\n",
    "print(tr_Y[1])\n",
    "print(tr_X[2][:150])\n",
    "print(tr_Y[2])\n",
    "print(tr_X[3][:150])\n",
    "print(tr_Y[3])\n",
    "print(tr_X[4][:150])\n",
    "print(tr_Y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we collect all the reviews from train validation and test set to generate \n",
    "texts = []\n",
    "texts += tr_X \n",
    "texts += te_X \n",
    "texts += val_X\n",
    "len(texts)\n",
    "\n",
    "#we clip the sentence length to first 250 words. \n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "#length of vocab, Tokenizer will only use vocab_len most common words\n",
    "vocab_len = 25000\n",
    "\n",
    "#we tokenize the texts and convert all the words to tokens\n",
    "tokenizer = Tokenizer(num_words=vocab_len)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "token_tr_X = tokenizer.texts_to_sequences(tr_X)\n",
    "token_te_X = tokenizer.texts_to_sequences(te_X)\n",
    "token_val_X = tokenizer.texts_to_sequences(val_X)\n",
    "\n",
    "#to ensure all reviews have the same length, we pad the smaller reviews with 0, \n",
    "#and cut the larger reviews to a max length \n",
    "#(we clip from the top, as the end of the reviews generally have a conclusion which provides better features)\n",
    "x_train = sequence.pad_sequences(token_tr_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = sequence.pad_sequences(token_te_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_val = sequence.pad_sequences(token_val_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "#changes the labels to one-hot encoding\n",
    "y_train = np_utils.to_categorical(tr_Y)\n",
    "y_test = np_utils.to_categorical(te_Y)\n",
    "y_val = np_utils.to_categorical(val_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20000, 250)\n",
      "X_test shape: (25000, 250)\n",
      "X_val shape: (5000, 250)\n",
      "y_train shape: (20000, 2)\n",
      "y_test shape: (25000, 2)\n",
      "y_val shape: (5000, 2)\n",
      "*****************************************\n",
      "Tokenized Reviews Train sets and their labels\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.  1.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.  1.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.  1.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 0.  1.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', x_train.shape)\n",
    "print('X_test shape:', x_test.shape)\n",
    "print('X_val shape:', x_val.shape)\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "\n",
    "\n",
    "print(\"*****************************************\")\n",
    "print(\"Tokenized Reviews Train sets and their labels\")\n",
    "print(x_train[0][:20])\n",
    "print(y_train[0])\n",
    "print()\n",
    "print(x_train[1][:20])\n",
    "print(y_train[1])\n",
    "print()\n",
    "print(x_train[2][:20])\n",
    "print(y_train[2])\n",
    "print()\n",
    "print(x_train[3][:20])\n",
    "print(y_train[3])\n",
    "print()\n",
    "print(x_train[4][:20])\n",
    "print(y_train[4])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************************************\n",
    "\n",
    "As you can see the reviews have now been transformed into indices to tokenized vocabulary and the labels have been converted to one-hot encoding. We can now go ahead and feed these sequences to Neural Network Models.\n",
    "\n",
    "********************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A\n",
    "\n",
    "Building your first model (5 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 402       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,600,602\n",
      "Trainable params: 9,600,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(25000, 128, input_length = 250))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## compile it here according to instructions\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 152s 8ms/step - loss: 0.4049 - acc: 0.7998 - val_loss: 0.2949 - val_acc: 0.8724\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 154s 8ms/step - loss: 0.0448 - acc: 0.9863 - val_loss: 0.4475 - val_acc: 0.8550\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 143s 7ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.5539 - val_acc: 0.8644\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 149s 7ms/step - loss: 3.2458e-04 - acc: 1.0000 - val_loss: 0.6099 - val_acc: 0.8682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fddb65ee390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B\n",
    "\n",
    "Stacking Fully Connected Layers (5 points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 402       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,640,802\n",
      "Trainable params: 9,640,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(25000, 128, input_length = 250))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## compile it here according to instructions\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 158s 8ms/step - loss: 0.4020 - acc: 0.8037 - val_loss: 0.2981 - val_acc: 0.8768\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 163s 8ms/step - loss: 0.0446 - acc: 0.9849 - val_loss: 0.4134 - val_acc: 0.8490\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 162s 8ms/step - loss: 0.0067 - acc: 0.9979 - val_loss: 0.6646 - val_acc: 0.8552\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 153s 8ms/step - loss: 0.0063 - acc: 0.9979 - val_loss: 0.7838 - val_acc: 0.8422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fddaea31a90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C\n",
    "\n",
    "Using LSTMS based networks(5 Points) \n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,348,354\n",
      "Trainable params: 3,348,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(25000, 128, input_length = 250))\n",
    "#model.add(Flatten())\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## compile it here according to instructions\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 490s 25ms/step - loss: 0.4418 - acc: 0.7871 - val_loss: 0.3692 - val_acc: 0.8620\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 486s 24ms/step - loss: 0.2176 - acc: 0.9170 - val_loss: 0.3466 - val_acc: 0.8588\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 482s 24ms/step - loss: 0.1158 - acc: 0.9603 - val_loss: 0.3748 - val_acc: 0.8670\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 491s 25ms/step - loss: 0.0778 - acc: 0.9738 - val_loss: 0.5051 - val_acc: 0.8254\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 487s 24ms/step - loss: 0.0843 - acc: 0.9698 - val_loss: 0.6400 - val_acc: 0.8492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fddab9cb1d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D\n",
    "\n",
    "Adding Pretrained Word Embeddings(10 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124252 unique tokens\n",
      "G Word embeddings: 1176088\n",
      "G Null word embeddings: 38684\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "#dimension of Glove Embeddings.\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "#load glove embeddings\n",
    "gembeddings_index = {}\n",
    "with codecs.open('glove.42B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        gembedding = np.asarray(values[1:], dtype='float32')\n",
    "        gembeddings_index[word] = gembedding\n",
    "#\n",
    "f.close()\n",
    "print('G Word embeddings:', len(gembeddings_index))\n",
    "\n",
    "# nb_words contains the total length of vocab\n",
    "nb_words = len(word_index) +1\n",
    "\n",
    "#get glove embeddings for each word in tokenizer.\n",
    "#g_word_embedding_matrix holds the embeddings dictionary\n",
    "g_word_embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    gembedding_vector = gembeddings_index.get(word)\n",
    "    if gembedding_vector is not None:\n",
    "        g_word_embedding_matrix[i] = gembedding_vector\n",
    "        \n",
    "#total words in the tokenizer not in Embedding matrix\n",
    "print('G Null word embeddings: %d' % np.sum(np.sum(g_word_embedding_matrix, axis=1) == 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         37275900  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 37,512,318\n",
      "Trainable params: 37,512,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, 300, weights = [g_word_embedding_matrix]))\n",
    "model.add(LSTM(128, recurrent_dropout = 0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "## to use the glove embeddings, your embedding layer would take the vocab size as input dimension, \n",
    "## Glove embedding dimension as the output dimsion\n",
    "## and you will provide the  embedding dictionary as the 'weights' parameter (!important) to the embedding layer.\n",
    "\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 412s 21ms/step - loss: 0.4106 - acc: 0.8123 - val_loss: 0.3109 - val_acc: 0.8696\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 412s 21ms/step - loss: 0.2014 - acc: 0.9229 - val_loss: 0.2538 - val_acc: 0.8978\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 411s 21ms/step - loss: 0.0882 - acc: 0.9716 - val_loss: 0.3356 - val_acc: 0.8860\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 411s 21ms/step - loss: 0.0342 - acc: 0.9906 - val_loss: 0.3951 - val_acc: 0.8876\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 411s 21ms/step - loss: 0.0180 - acc: 0.9945 - val_loss: 0.5655 - val_acc: 0.8888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1050eaee10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dont attempt this\n",
    "\n",
    "Stacking LSTM layers\n",
    "\n",
    "Unfortunately it takes very long to train, be aware we can stack LTMSs over each other like this.\n",
    "This requires bottom LSTM to return a sequences instead instead of single vector, which becomes input for the top LSTM.\n",
    "\n",
    "\n",
    "![title](img/model5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E\n",
    "\n",
    "Using Convolutional Networks (10 points)\n",
    "\n",
    "Construct the model, shown below. Use the same loss functions and optimizers as before\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model6.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 248, 128)          115328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 246, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1999104   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 39,421,662\n",
      "Trainable params: 39,421,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, 300, weights = [g_word_embedding_matrix], input_length = 250))\n",
    "model.add(Conv1D(filters = 128, kernel_size = 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(filters = 32, kernel_size = 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 244s 12ms/step - loss: 0.4461 - acc: 0.7661 - val_loss: 0.2576 - val_acc: 0.8978\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 244s 12ms/step - loss: 0.2204 - acc: 0.9153 - val_loss: 0.2505 - val_acc: 0.8978\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 245s 12ms/step - loss: 0.1235 - acc: 0.9532 - val_loss: 0.2973 - val_acc: 0.8912\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 243s 12ms/step - loss: 0.0768 - acc: 0.9730 - val_loss: 0.3269 - val_acc: 0.8920\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 244s 12ms/step - loss: 0.0446 - acc: 0.9838 - val_loss: 0.6186 - val_acc: 0.8720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1050454cc0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part F\n",
    "\n",
    "Model constructed : (5 points)\n",
    "\n",
    "Test Accuracy Over 87.5%: (5 Points)\n",
    "\n",
    "Bonus: Min(10, Square of (test_score - 88%))\n",
    "\n",
    "Create your best model, use Validation score to judge your best model and check accuracy on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 248, 128)          115328    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 246, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 244, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               1999104   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 39,421,662\n",
      "Trainable params: 39,421,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here - experimenting with conv\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, 300, weights = [g_word_embedding_matrix], input_length = 250))\n",
    "model.add(Conv1D(filters = 128, kernel_size = 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(filters = 32, kernel_size = 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## to use the glove embeddings, your embedding layer would take the vocab size as input dimension, \n",
    "## Glove embedding dimension as the output dimsion\n",
    "## and you will provide the  embedding dictionary as the 'weights' parameter (!important) to the embedding layer.\n",
    "\n",
    "## compile it here according to instructions\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep saving models with different names in model_name, \n",
    "\n",
    "so you can retrieve their weights again for testing, you dont have to retrain \n",
    "(You would have to initialize the model definition again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/7\n",
      "20000/20000 [==============================] - 247s 12ms/step - loss: 0.4384 - acc: 0.7780 - val_loss: 0.3090 - val_acc: 0.8702\n",
      "Epoch 2/7\n",
      "20000/20000 [==============================] - 246s 12ms/step - loss: 0.2313 - acc: 0.9094 - val_loss: 0.2426 - val_acc: 0.9040\n",
      "Epoch 3/7\n",
      "20000/20000 [==============================] - 244s 12ms/step - loss: 0.1383 - acc: 0.9485 - val_loss: 0.2739 - val_acc: 0.9036\n",
      "Epoch 4/7\n",
      "20000/20000 [==============================] - 245s 12ms/step - loss: 0.0857 - acc: 0.9689 - val_loss: 0.2900 - val_acc: 0.9022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1871454978>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_dir = \"./weights/\"\n",
    "model_name = 'model_best'\n",
    "early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "bst_model_path = wt_dir + model_name + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, monitor='val_acc', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=7,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True,\n",
    "         callbacks=[early_stopping, model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan on using Ensemble averaging, feel free to edit the code below or add multiple models.\n",
    "\n",
    "Make sure they get saved and can be retrieved when executing serially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 34s 1ms/step\n",
      "Accuracy: 89.57%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(bst_model_path)\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Conv1D model - stored as model1.h5 in weights directory - 89.57%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part G\n",
    "\n",
    "Explain how Dense, LSTM and Convolution Layers work.\n",
    "\n",
    "Explain Relu, Dropout, and Softmax work.\n",
    "\n",
    "Analyze the architectures you constructed, with the accuracies you achieved and the training time it took. \n",
    "\n",
    "What are some insights you gained with these experiments? \n",
    "\n",
    "(5 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "\n",
    "##### 1.Dense Layers: \n",
    "A dense layer is a layer of neurons which is fully connected. A linear operation in which every input is connected to every output by a weight (so there are n_inputs * n_outputs weights); followed by a non-linear activation function. The values in the weight matrix are the trainable parameters which get updated during backpropagation. A dense layer thus is used to change the dimensions of the input vector. Mathematically, it applies a rotation, scaling, translation transform to the vector.\n",
    "\n",
    "In Keras, Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (if use_bias is True).\n",
    " \n",
    " \n",
    "##### 2.LSTM Layers: \n",
    "Long short-term memory (LSTM) block or network is a simple recurrent neural network which can be used as a building component or block (of hidden layers) for an eventually bigger recurrent neural network.The cell is responsible for \"remembering\" values over arbitrary time intervals. An LSTM block is composed of four main components: a cell, an input gate, an output gate and a forget gate. The cell is responsible for \"remembering\" values over arbitrary time intervals. Each of the three gates can be thought as a \"conventional\" artificial neuron, as in a multi-layer (or feedforward) neural network: that is, they compute an activation of a weighted sum. There are connections between these gates and the cell. Some of the connections are recurrent, some of them are not. An LSTM is well-suited to classify, process and predict time series given time lags of unknown size and duration between important events.\n",
    "\n",
    "\n",
    "##### 3.Convolution Layers: \n",
    "A linear operation using a subset of the weights of a dense layer. Nearby inputs are connected to nearby outputs. The weights for the convolutions at each location are shared. Due to the weight sharing, and the use of a subset of the weights of a dense layer, there's far less weights than in a dense layer. They are followed by a non-linear activation function. This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs (1-D convolutionl layer). If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well. \n",
    "\n",
    "\n",
    "##### 4. Relu (Rectified Linear unit): \n",
    "Relu is a commonly used activation functions in neural nets. The ReLU function is f(x)=max(0,x). Usually this is applied element-wise to the output of some other function, such as a matrix-vector product. In MLP usages, rectifier units replace all other activation functions except perhaps the output layer. \n",
    "One way ReLUs improve neural networks is by speeding up training. The gradient computation is very simple (either 0 or 1 depending on the sign of xx). Also, the computational step of a ReLU is easy: any negative elements are set to 0.0. Logistic and hyperbolic tangent networks suffer from the vanishing gradient problem, where the gradient essentially becomes 0 after a certain amount of training (because of the two horizontal asymptotes) and stops all learning in that section of the network. ReLU units are only 0 gradient on one side, which empirically is superior.\n",
    "\n",
    "\n",
    "##### 5. Softmax:\n",
    "The softmax function is commonly used in the final layer of a neural network-based classifier. Such networks are commonly trained under a log loss (or cross-entropy) regime, giving a non-linear variant of multinomial logistic regression. softmax function can be used to represent a categorical distribution – that is, a probability distribution over K different possible outcomes.The function itself is a generalization of the logistic function that \"squashes\" a K-dimensional vector of arbitrary real values to a K-dimensional vector of real values in the range [0, 1] that add up to 1. \n",
    "\n",
    "\n",
    "##### 6. Dropout: \n",
    "Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.Weights of neurons are tuned for specific features providing some specialization. Neighboring neurons become to rely on this specialization, which if taken too far can result in a fragile model too specialized to the training data. If neurons are randomly dropped out of the network during training, that other neurons will have to step in and handle the representation required to make predictions for the missing neurons. The effect is that the network becomes less sensitive to the specific weights of neurons. This in turn results in a network that is capable of better generalization and is less likely to overfit the training data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of architectures & Insights\n",
    "Part A creates a very simple model with 3 layers. The embedding layer serves to create an input layer by creating embeddings of size 128 which is then flattened to create a layer of 32000 neurons to represent the input. There is dense hidden layer of 200 neurons with a relu activation. The output layer is a dense layer with 2 output neurons and softmax activation to determine the probability of being a positive or negative review. There are 9,600,602 trainable parameters.Each of the 4 epochs take 150secs and validation accuracy increases with increasing epochs. Max val accuracy though is after 1st epoch at 0.87.\n",
    "\n",
    "\n",
    "PartB creates a similar network with an additional hidden dense layer of 200 neurons with relu activation. There epochs seem to take slightly longer to train, though overall validation accuracy does not seem to increase as compared to PartA. After 4 epochs val accuracy is actually lower than PartA and the model seem to be overfitting.No. of trainable parameters is 9,640,802. The additional 40,000 neurons take more time to train, add complexity but do not improve accuracy.\n",
    "\n",
    "\n",
    "PartC uses an LSTM layer of size 128 and a dense layer of size 128 as hidden layers.No. of trainable parameters is 3,348,354 which is much lesser than both part A and B. However this model takes much longer to train (~485secs per epoch). Val accuracy after first epoch is similar to PartA and PartB. After 5 epochs its validation accuracy is still lower than the simplest net in Part A.\n",
    "\n",
    "\n",
    "PartD is a more complex model and uses 300 dimensional Glove word embeddings for input. It uses an LSTM layer with recurrent dropout and an additional Dropout layer in comparison to partC. No. of trainable parameters increase massively to 37M. However each epoch takes approx. 411 secs to train which is faster than PartC (because of recurrent dropout and dropout). Val accuracy increases as compared to previous models and is maximum after 2nd epoch at 0.897.\n",
    "\n",
    "\n",
    "PartE uses a much more complex model with ~39M trainabale parameters.It uses 3 convolutional 1D layers of decreasing filter size and 4 dropout layers of 20%. Hidden layers use relu activation and output uses softmax as with the other cases.On average each epoch takes 244secs to train, which is much lower than part C and D despite more trainable parameters. Model has highest accuracy after first 2 epochs of 0.897 after which val accuracy starts going down (due to overfitting to training set).\n",
    "\n",
    "It seems in most cases, the validation accuracy seems to go down after first 1-2 epochs due to overfitting to training set. Also simply adding more dense layers merely increases training time but does not give significant increase in accuracy. LSTM models take a very long time to train. In our case, Part E with dropout and convolutional layers gave best performance. 1D convolutional layers were thus used in the best model similar to PartE. Adding more dense layers and increasing dropout did not improve performance. Best accuracy obtained on test set was 89.57%. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
